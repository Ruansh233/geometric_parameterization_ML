{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe9e5fd1fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import copy\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Prepare Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data preparation steps are done here\n",
    "num_simulations = 100\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Load npy data from training_data folder\n",
    "pressure_data = np.load('training_data/pressure_data.npy')\n",
    "velocity_data = np.load('training_data/velocity_data.npy')\n",
    "\n",
    "pressure_flattened = pressure_data.reshape(num_simulations, -1)\n",
    "velocity_flattened = velocity_data.reshape(num_simulations, -1)\n",
    "\n",
    "data_flattened = np.hstack((pressure_flattened, velocity_flattened))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(data_flattened, dtype=torch.float32)\n",
    "\n",
    "# Split the data\n",
    "train_size = int(train_ratio * num_simulations)\n",
    "test_size = num_simulations - train_size\n",
    "train_dataset, test_dataset = random_split(data_tensor, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Define the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "# Define model, loss, and optimizer\n",
    "input_dim = data_flattened.shape[1]  # 15000\n",
    "latent_dim = 256\n",
    "autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "model_save_path = 'model_saved'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2053, Val Loss: 0.0778\n",
      "Epoch 2/20, Train Loss: 0.0549, Val Loss: 0.0430\n",
      "Epoch 3/20, Train Loss: 0.0480, Val Loss: 0.0563\n",
      "Epoch 4/20, Train Loss: 0.0570, Val Loss: 0.0578\n",
      "Epoch 5/20, Train Loss: 0.0574, Val Loss: 0.0561\n",
      "Epoch 6/20, Train Loss: 0.0554, Val Loss: 0.0531\n",
      "Epoch 7/20, Train Loss: 0.0522, Val Loss: 0.0491\n",
      "Epoch 8/20, Train Loss: 0.0462, Val Loss: 0.0281\n",
      "Epoch 9/20, Train Loss: 0.0379, Val Loss: 0.0258\n",
      "Epoch 10/20, Train Loss: 0.0277, Val Loss: 0.0267\n",
      "Epoch 11/20, Train Loss: 0.0239, Val Loss: 0.0198\n",
      "Epoch 12/20, Train Loss: 0.0189, Val Loss: 0.0167\n",
      "Epoch 13/20, Train Loss: 0.0163, Val Loss: 0.0164\n",
      "Epoch 14/20, Train Loss: 0.0163, Val Loss: 0.0162\n",
      "Epoch 15/20, Train Loss: 0.0160, Val Loss: 0.0157\n",
      "Epoch 16/20, Train Loss: 0.0155, Val Loss: 0.0152\n",
      "Epoch 17/20, Train Loss: 0.0151, Val Loss: 0.0150\n",
      "Epoch 18/20, Train Loss: 0.0148, Val Loss: 0.0147\n",
      "Epoch 19/20, Train Loss: 0.0147, Val Loss: 0.0146\n",
      "Epoch 20/20, Train Loss: 0.0145, Val Loss: 0.0145\n"
     ]
    }
   ],
   "source": [
    "# Training the Autoencoder\n",
    "num_epochs = 20\n",
    "best_model_wts = copy.deepcopy(autoencoder.state_dict())\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        inputs = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(autoencoder.state_dict())\n",
    "\n",
    "# Load the best model weights\n",
    "autoencoder.load_state_dict(best_model_wts)\n",
    "\n",
    "# Save the trained autoencoder\n",
    "torch.save(autoencoder.state_dict(), f'{model_save_path}/autoencoder_10epoches.pth')\n",
    "\n",
    "# Extract the encoder\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Extract and store the decoder weights\n",
    "decoder_weights = []\n",
    "for i in range(num_simulations):\n",
    "    autoencoder.decoder.load_state_dict(autoencoder.decoder.state_dict())\n",
    "    decoder_weights.append(copy.deepcopy(autoencoder.decoder.state_dict()))\n",
    "\n",
    "# Save the decoder weights for future use\n",
    "torch.save(decoder_weights, f'{model_save_path}/decoder_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interpolate on the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load design parameters from training_data folder\n",
    "design_parameters = np.load('training_data/displacement_data.npy')\n",
    "design_parameters_tensor = torch.tensor(design_parameters, dtype=torch.float32)\n",
    "\n",
    "# Load the trained autoencoder\n",
    "new_autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "new_autoencoder.load_state_dict(torch.load(f'{model_save_path}/autoencoder_10epoches.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a empty numpy array with the shape of the training data\n",
    "latent_np = np.empty((train_size, latent_dim))\n",
    "\n",
    "# Obtain the latent space\n",
    "with torch.no_grad():\n",
    "    latent_space = torch.empty(0, latent_dim)\n",
    "    for data in train_loader:\n",
    "        np.append(latent_np, new_autoencoder.encoder(data).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load design parameters from training_data folder\n",
    "design_parameters = np.load('training_data/displacement_data.npy')\n",
    "\n",
    "train_design_parameters = design_parameters[train_dataset.indices]\n",
    "test_design_parameters = design_parameters[test_dataset.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New design parameter: 11\n",
      "MSE Loss: 0.1395379602909088\n"
     ]
    }
   ],
   "source": [
    "# randomly select a design parameter from the test set\n",
    "new_index = np.random.randint(0, test_size)\n",
    "new_design_parameter = test_design_parameters[new_index].reshape(1, -1)\n",
    "new_dataset = test_dataset[new_index]\n",
    "\n",
    "print(f'New design parameter: {new_index}')\n",
    "\n",
    "# Compute the interpolation weights\n",
    "interpolation_weights = np.matmul(train_design_parameters, new_design_parameter.T)\n",
    "\n",
    "# Interpolate the latent space\n",
    "interpolated_latent = np.matmul(latent_np.T, interpolation_weights)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = new_autoencoder.decoder(torch.tensor(interpolated_latent.T, dtype=torch.float32))\n",
    "\n",
    "# Compute the MSE loss between the original and reconstructed data\n",
    "loss = criterion(reconstructed, new_dataset.reshape(1, -1))\n",
    "print(f'MSE Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. An alternative method, interpolate on the latent space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = interp1d(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomParametric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
