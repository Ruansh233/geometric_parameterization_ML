{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f70665e5fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import copy\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Prepare Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data preparation steps are done here\n",
    "num_simulations = 100\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Load npy data from training_data folder\n",
    "pressure_data = np.load('training_data/pressure_data.npy')\n",
    "velocity_data = np.load('training_data/velocity_data.npy')\n",
    "\n",
    "pressure_flattened = pressure_data.reshape(num_simulations, -1)\n",
    "velocity_flattened = velocity_data.reshape(num_simulations, -1)\n",
    "\n",
    "data_flattened = np.hstack((pressure_flattened, velocity_flattened))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(data_flattened, dtype=torch.float32)\n",
    "\n",
    "# Split the data\n",
    "train_size = int(train_ratio * num_simulations)\n",
    "test_size = num_simulations - train_size\n",
    "train_dataset, test_dataset = random_split(data_tensor, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Define the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "# Define model, loss, and optimizer\n",
    "input_dim = data_flattened.shape[1]  # 15000\n",
    "latent_dim = 256\n",
    "autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2073, Val Loss: 0.0945\n",
      "Epoch 2/10, Train Loss: 0.0609, Val Loss: 0.0413\n",
      "Epoch 3/10, Train Loss: 0.0446, Val Loss: 0.0514\n",
      "Epoch 4/10, Train Loss: 0.0510, Val Loss: 0.0487\n",
      "Epoch 5/10, Train Loss: 0.0473, Val Loss: 0.0429\n",
      "Epoch 6/10, Train Loss: 0.0410, Val Loss: 0.0336\n",
      "Epoch 7/10, Train Loss: 0.0292, Val Loss: 0.0240\n",
      "Epoch 8/10, Train Loss: 0.0223, Val Loss: 0.0204\n",
      "Epoch 9/10, Train Loss: 0.0191, Val Loss: 0.0177\n",
      "Epoch 10/10, Train Loss: 0.0172, Val Loss: 0.0158\n"
     ]
    }
   ],
   "source": [
    "# Training the Autoencoder\n",
    "num_epochs = 10\n",
    "best_model_wts = copy.deepcopy(autoencoder.state_dict())\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        inputs = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(autoencoder.state_dict())\n",
    "\n",
    "# Load the best model weights\n",
    "autoencoder.load_state_dict(best_model_wts)\n",
    "\n",
    "# Extract the encoder\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Save the trained autoencoder\n",
    "torch.save(autoencoder.state_dict(), 'model_saved/autoencoder_10epoches.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.0158, Val Loss: 0.0156\n",
      "Epoch 2/20, Train Loss: 0.0155, Val Loss: 0.0155\n",
      "Epoch 3/20, Train Loss: 0.0152, Val Loss: 0.0150\n",
      "Epoch 4/20, Train Loss: 0.0149, Val Loss: 0.0148\n",
      "Epoch 5/20, Train Loss: 0.0148, Val Loss: 0.0147\n",
      "Epoch 6/20, Train Loss: 0.0146, Val Loss: 0.0146\n",
      "Epoch 7/20, Train Loss: 0.0145, Val Loss: 0.0146\n",
      "Epoch 8/20, Train Loss: 0.0145, Val Loss: 0.0145\n",
      "Epoch 9/20, Train Loss: 0.0144, Val Loss: 0.0145\n",
      "Epoch 10/20, Train Loss: 0.0144, Val Loss: 0.0144\n",
      "Epoch 11/20, Train Loss: 0.0144, Val Loss: 0.0144\n",
      "Epoch 12/20, Train Loss: 0.0144, Val Loss: 0.0144\n",
      "Epoch 13/20, Train Loss: 0.0144, Val Loss: 0.0144\n",
      "Epoch 14/20, Train Loss: 0.0144, Val Loss: 0.0144\n",
      "Epoch 15/20, Train Loss: 0.0144, Val Loss: 0.0144\n",
      "Epoch 16/20, Train Loss: 0.0143, Val Loss: 0.0144\n",
      "Epoch 17/20, Train Loss: 0.0143, Val Loss: 0.0144\n",
      "Epoch 18/20, Train Loss: 0.0143, Val Loss: 0.0144\n",
      "Epoch 19/20, Train Loss: 0.0143, Val Loss: 0.0144\n",
      "Epoch 20/20, Train Loss: 0.0143, Val Loss: 0.0144\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        inputs = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(autoencoder.state_dict())\n",
    "\n",
    "# Load the best model weights\n",
    "autoencoder.load_state_dict(best_model_wts)\n",
    "\n",
    "# Extract the encoder\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Save the trained autoencoder\n",
    "torch.save(autoencoder.state_dict(), 'model_saved/autoencoder_30epoches.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m latent_train \u001b[38;5;241m=\u001b[39m encoder(train_dataset\u001b[38;5;241m.\u001b[39mdataset[:train_size])\n\u001b[1;32m      9\u001b[0m latent_test \u001b[38;5;241m=\u001b[39m encoder(test_dataset\u001b[38;5;241m.\u001b[39mdataset[:test_size])\n\u001b[0;32m---> 11\u001b[0m train_design_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesign_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_train\u001b[49m\u001b[43m)\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m test_design_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(design_test, latent_test), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPredictor\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/miniconda3/envs/geomParametric/lib/python3.12/site-packages/torch/utils/data/dataset.py:203\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m~/miniconda3/envs/geomParametric/lib/python3.12/site-packages/torch/utils/data/dataset.py:204\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 204\u001b[0m         \u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    205\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Load design parameters from training_data folder\n",
    "design_parameters = np.load('training_data/displacement_data.npy')\n",
    "design_parameters_tensor = torch.tensor(design_parameters, dtype=torch.float32)\n",
    "\n",
    "# Split the design parameters\n",
    "design_train, design_test = random_split(design_parameters_tensor, [train_size, test_size])\n",
    "\n",
    "# Function to extract tensors from subsets\n",
    "def get_subset_tensors(subset):\n",
    "    return subset.dataset[subset.indices]\n",
    "\n",
    "latent_train = encoder(get_subset_tensors(train_dataset))\n",
    "latent_test = encoder(get_subset_tensors(test_dataset))\n",
    "\n",
    "train_design_loader = DataLoader(TensorDataset(design_train, latent_train), batch_size=32, shuffle=True)\n",
    "test_design_loader = DataLoader(TensorDataset(design_test, latent_test), batch_size=32, shuffle=False)\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "predictor = Predictor(design_parameters.shape[1], latent_dim)\n",
    "predictor_criterion = nn.MSELoss()\n",
    "predictor_optimizer = optim.Adam(predictor.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Predictor\n",
    "best_predictor_wts = copy.deepcopy(predictor.state_dict())\n",
    "best_predictor_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    predictor.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for design_data, latent_data in train_design_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = predictor(design_data)\n",
    "        loss = criterion(outputs, latent_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * design_data.size(0)\n",
    "\n",
    "    train_loss /= len(train_design_loader.dataset)\n",
    "\n",
    "    predictor.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for design_data, latent_data in test_design_loader:\n",
    "            outputs = predictor(design_data)\n",
    "            loss = criterion(outputs, latent_data)\n",
    "            val_loss += loss.item() * design_data.size(0)\n",
    "\n",
    "    val_loss /= len(test_design_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_predictor_loss:\n",
    "        best_predictor_loss = val_loss\n",
    "        best_predictor_wts = copy.deepcopy(predictor.state_dict())\n",
    "\n",
    "# Load the best model weights\n",
    "predictor.load_state_dict(best_predictor_wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Predict Flow Fields for New Design Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict flow fields for new design parameters\n",
    "new_design_parameters = np.random.rand(1, design_parameters.shape[1])\n",
    "new_design_parameters_tensor = torch.tensor(new_design_parameters, dtype=torch.float32)\n",
    "\n",
    "# Predict latent space representation\n",
    "latent_pred_new = predictor(new_design_parameters_tensor)\n",
    "\n",
    "# Decode the latent representation to obtain the flow fields\n",
    "predicted_flow_fields = autoencoder.decoder(latent_pred_new)\n",
    "predicted_flow_fields = predicted_flow_fields.detach().numpy().reshape(5000, 3)\n",
    "\n",
    "print(predicted_flow_fields)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomParametric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
