{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezyrb import POD, AE, GPR, ANN, RBF, Database\n",
    "from ezyrb import ReducedOrderModel as ROM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "np.bool = np.bool_ # to avoid error in pyvista\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from varname import nameof\n",
    "import pyvista as pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read and preprocess of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data preparation steps are done here\n",
    "num_simulations = 200\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Load npy data from training_data folder\n",
    "pressure_data = np.load('training_data/pressure_data_concat.npy')\n",
    "velocity_data = np.load('training_data/velocity_data_concat.npy')\n",
    "points_data = np.load('training_data/points_data.npy')\n",
    "\n",
    "# Load design parameters from training_data folder\n",
    "design_parameters = np.load('training_data/displacement_data_concat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# randomly spit the data into training and testing data\n",
    "train_pressure, test_pressure = train_test_split(pressure_data, train_size=train_ratio, random_state=random_state)\n",
    "train_velocity, test_velocity = train_test_split(velocity_data, train_size=train_ratio, random_state=random_state)\n",
    "train_design_parameters, test_design_parameters = train_test_split(design_parameters, train_size=train_ratio, random_state=random_state)\n",
    "train_points, test_points = train_test_split(points_data, train_size=train_ratio, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns with all zeros in train_design_parameters\n",
    "eliminate_columns = np.all(train_design_parameters == 0, axis=0)\n",
    "train_design_parameters = train_design_parameters[:, ~eliminate_columns]\n",
    "test_design_parameters = test_design_parameters[:, ~eliminate_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PODI_rom(ae, model, train_design_parameters, train_data):\n",
    "    # Define the database\n",
    "    db = Database(train_design_parameters, train_data)\n",
    "    # Define the ROM\n",
    "    rom = ROM(db, ae, model)\n",
    "    rom.fit()\n",
    "\n",
    "    return rom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RBF interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch      1]\t2.042527e-01\n",
      "[epoch     50]\t6.480517e-04\n"
     ]
    }
   ],
   "source": [
    "# Evaluate AE on the pressure data\n",
    "low_dim = 20\n",
    "optim = torch.optim.Adam\n",
    "encoder_list = [2000, 200, low_dim]\n",
    "decoder_list = [low_dim, 200, 2000]\n",
    "encoder_activation = torch.nn.Tanh\n",
    "decoder_activation = torch.nn.Tanh\n",
    "\n",
    "p_ae = AE(encoder_list, decoder_list, encoder_activation(), decoder_activation(), \n",
    "          [2000, 1e-5], optimizer=optim, frequency_print=50)\n",
    "\n",
    "# Define the ANN model\n",
    "p_ann = ann = ANN([5, 20], nn.Tanh(), [2000,1e-5])\n",
    "\n",
    "# Perform PODI on the pressure data\n",
    "p_rom = PODI_rom(p_ae, p_ann, train_design_parameters, train_pressure)\n",
    "\n",
    "# Predict the pressure data\n",
    "p_predict = p_rom.predict(test_design_parameters)\n",
    "\n",
    "# calculate the reconstrcuted error for each test data using mean squared error\n",
    "p_reconstructed_error = np.linalg.norm(test_pressure - p_predict.snapshots_matrix, axis=1) / np.linalg.norm(test_pressure, axis=1)\n",
    "\n",
    "# plot the error\n",
    "plt.bar([i for i in range(len(p_reconstructed_error))], p_reconstructed_error)\n",
    "plt.title('Pressure reconstructed error')\n",
    "plt.xlabel('Test data')\n",
    "plt.ylabel('Reconstructed error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch      1]\t1.844543e-01\n",
      "[epoch     20]\t5.797183e-03\n",
      "[epoch     40]\t1.995192e-03\n",
      "[epoch     60]\t1.475034e-03\n",
      "[epoch     80]\t1.406138e-03\n"
     ]
    }
   ],
   "source": [
    "# Evaluate AE on the pressure data\n",
    "low_dim = 50\n",
    "optim = torch.optim.Adam\n",
    "encoder_list = [2000, 100, low_dim]\n",
    "decoder_list = [low_dim, 1000, 2000]\n",
    "encoder_activation = torch.nn.Tanh\n",
    "decoder_activation = torch.nn.Tanh\n",
    "\n",
    "u_ae = AE(encoder_list, decoder_list, encoder_activation(), decoder_activation(), \n",
    "          [1000, 1e-5], optimizer=optim, frequency_print=20)\n",
    "\n",
    "# Define the ANN model\n",
    "u_ann = ANN([5, 20], nn.Tanh(), [2000,1e-5])\n",
    "\n",
    "# Perform PODI on the pressure data\n",
    "u_rom = PODI_rom(u_ae, u_ann, train_design_parameters, train_velocity)\n",
    "\n",
    "# Predict the pressure data\n",
    "u_predict = u_rom.predict(test_design_parameters)\n",
    "\n",
    "# calculate the reconstrcuted error for each test data using mean squared error\n",
    "u_reconstructed_error = np.linalg.norm(test_velocity - u_predict.snapshots_matrix, axis=1) / np.linalg.norm(test_velocity, axis=1)\n",
    "\n",
    "# plot the error\n",
    "plt.bar([i for i in range(len(u_reconstructed_error))], u_reconstructed_error)\n",
    "plt.title('Velocity reconstructed error')\n",
    "plt.xlabel('Test data')\n",
    "plt.ylabel('Reconstructed error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VTK visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a funtion to write the data into a VTK file\n",
    "def vtk_writer(field_data, field_name, data_type,\n",
    "               refVTM, save_path_name, points_data = None):\n",
    "    # Add velocity data to each block within the MultiBlock dataset\n",
    "    \n",
    "    for block_i in range(refVTM.n_blocks):\n",
    "        block = refVTM[block_i]\n",
    "        if block is not None:\n",
    "            if data_type == 'scalar':\n",
    "                for data_i in range(len(field_name)):\n",
    "                    block.cell_data[field_name[data_i]] = field_data[data_i]\n",
    "            elif data_type == 'vector':\n",
    "                for data_i in range(len(field_name)):\n",
    "                    field = field_data[data_i].reshape(3, -1).T\n",
    "                    block.cell_data[field_name[data_i]] = field \n",
    "            if points_data is not None:\n",
    "                points = points_data.reshape(3, -1).T\n",
    "                block.points = points\n",
    "\n",
    "    # Save the modified VTM file\n",
    "    output_vtm_file_path = f'{save_path_name}.vtm'\n",
    "    refVTM.save(output_vtm_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the truth data, reconstructed data, and error into a VTK file\n",
    "# Load the reference mesh VTM file\n",
    "refVTM = pv.MultiBlock('simulation_data/refCase/VTK/refCase_0.vtm')\n",
    "field_name = ['truth', 'reconstructed', 'error']\n",
    "p_error_field = test_pressure - p_predict.snapshots_matrix\n",
    "\n",
    "# loop all test data and write the data into VTK file\n",
    "for i in range(len(test_pressure)):\n",
    "    vtk_writer([test_pressure[i], p_predict.snapshots_matrix[i], p_error_field[i]], \n",
    "                field_name, 'scalar', refVTM, f'visualization_data/EZyRB_PODI_GPR/test_case_{i}_pressure', test_points[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the velocity data into VTK file\n",
    "field_name = ['truth', 'reconstructed', 'error']\n",
    "u_error_field = test_velocity - u_predict.snapshots_matrix\n",
    "\n",
    "# loop all test data and write the data into VTK file\n",
    "for i in range(len(test_velocity)):\n",
    "    vtk_writer([test_velocity[i], u_predict.snapshots_matrix[i], u_error_field[i]], \n",
    "                field_name, 'vector', refVTM, f'visualization_data/EZyRB_PODI_GPR/test_case_{i}_velocity', test_points[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomParametric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
