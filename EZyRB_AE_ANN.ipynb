{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezyrb import POD, AE, GPR, ANN, RBF, Database\n",
    "from ezyrb import ReducedOrderModel as ROM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "np.bool = np.bool_ # to avoid error in pyvista\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from varname import nameof\n",
    "import pyvista as pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read and preprocess of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data preparation steps are done here\n",
    "num_simulations = 200\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Load npy data from training_data folder\n",
    "pressure_data = np.load('training_data/pressure_data_concat.npy')\n",
    "velocity_data = np.load('training_data/velocity_data_concat.npy')\n",
    "points_data = np.load('training_data/points_data.npy')\n",
    "\n",
    "# Load design parameters from training_data folder\n",
    "design_parameters = np.load('training_data/displacement_data_concat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# randomly spit the data into training and testing data\n",
    "train_pressure, test_pressure = train_test_split(pressure_data, train_size=train_ratio, random_state=random_state)\n",
    "train_velocity, test_velocity = train_test_split(velocity_data, train_size=train_ratio, random_state=random_state)\n",
    "train_design_parameters, test_design_parameters = train_test_split(design_parameters, train_size=train_ratio, random_state=random_state)\n",
    "train_points, test_points = train_test_split(points_data, train_size=train_ratio, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns with all zeros in train_design_parameters\n",
    "eliminate_columns = np.all(train_design_parameters == 0, axis=0)\n",
    "train_design_parameters = train_design_parameters[:, ~eliminate_columns]\n",
    "test_design_parameters = test_design_parameters[:, ~eliminate_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PODI_rom(ae, model, train_design_parameters, train_data):\n",
    "    # Define the database\n",
    "    db = Database(train_design_parameters, train_data)\n",
    "    # Define the ROM\n",
    "    rom = ROM(db, ae, model)\n",
    "    rom.fit()\n",
    "\n",
    "    return rom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ANN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m p_ann \u001b[38;5;241m=\u001b[39m ann \u001b[38;5;241m=\u001b[39m ANN([\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m], nn\u001b[38;5;241m.\u001b[39mTanh(), [\u001b[38;5;241m2000\u001b[39m,\u001b[38;5;241m1e-5\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Perform PODI on the pressure data\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m p_rom \u001b[38;5;241m=\u001b[39m \u001b[43mPODI_rom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_ae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_ann\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_design_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pressure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Predict the pressure data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m p_predict \u001b[38;5;241m=\u001b[39m p_rom\u001b[38;5;241m.\u001b[39mpredict(test_design_parameters)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mPODI_rom\u001b[0;34m(ae, model, train_design_parameters, train_data)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the ROM\u001b[39;00m\n\u001b[1;32m      5\u001b[0m rom \u001b[38;5;241m=\u001b[39m ROM(db, ae, model)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rom\n",
      "File \u001b[0;32m~/miniconda3/envs/ezyrb_env/lib/python3.12/site-packages/ezyrb/reducedordermodel.py:74\u001b[0m, in \u001b[0;36mReducedOrderModel.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m plugin \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplugins:\n\u001b[1;32m     72\u001b[0m     plugin\u001b[38;5;241m.\u001b[39mfom_preprocessing(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnapshots_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# print(self.reduction.singular_values)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# print(self._full_database.snapshots_matrix)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m reduced_snapshots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_database\u001b[38;5;241m.\u001b[39msnapshots_matrix\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/miniconda3/envs/ezyrb_env/lib/python3.12/site-packages/ezyrb/reduction/ae.py:163\u001b[0m, in \u001b[0;36mAE.fit\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m flag:\n\u001b[1;32m    161\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(values))\n\u001b[0;32m--> 163\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    166\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ezyrb_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:98\u001b[0m, in \u001b[0;36mL1Loss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ezyrb_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
      "File \u001b[0;32m~/miniconda3/envs/ezyrb_env/lib/python3.12/site-packages/torch/nn/_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[1;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# Evaluate AE on the pressure data\n",
    "low_dim = 20\n",
    "optim = torch.optim.Adam\n",
    "encoder_list = [3000, 500, 100, low_dim]\n",
    "decoder_list = [low_dim, 1000]\n",
    "encoder_activation = torch.nn.Tanh\n",
    "decoder_activation = torch.nn.Tanh\n",
    "\n",
    "p_ae = AE(encoder_list, decoder_list, encoder_activation(), decoder_activation(), \n",
    "          [500, 1e-5], optimizer=optim, frequency_print=50, loss=torch.nn.CrossEntropyLoss)\n",
    "\n",
    "# Define the ANN model\n",
    "p_ann = ann = ANN([5, 20], nn.Tanh(), [2000,1e-5])\n",
    "\n",
    "# Perform PODI on the pressure data\n",
    "p_rom = PODI_rom(p_ae, p_ann, train_design_parameters, train_pressure)\n",
    "\n",
    "# Predict the pressure data\n",
    "p_predict = p_rom.predict(test_design_parameters)\n",
    "\n",
    "# calculate the reconstrcuted error for each test data using mean squared error\n",
    "p_reconstructed_error = np.linalg.norm(test_pressure - p_predict.snapshots_matrix, axis=1) / np.linalg.norm(test_pressure, axis=1)\n",
    "\n",
    "# plot the error\n",
    "plt.bar([i for i in range(len(p_reconstructed_error))], p_reconstructed_error)\n",
    "plt.title('Pressure reconstructed error')\n",
    "plt.xlabel('Test data')\n",
    "plt.ylabel('Reconstructed error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch      1]\t1.844543e-01\n",
      "[epoch     20]\t5.797183e-03\n",
      "[epoch     40]\t1.995192e-03\n",
      "[epoch     60]\t1.475034e-03\n",
      "[epoch     80]\t1.406138e-03\n"
     ]
    }
   ],
   "source": [
    "# Evaluate AE on the pressure data\n",
    "low_dim = 50\n",
    "optim = torch.optim.Adam\n",
    "encoder_list = [2000, 100, low_dim]\n",
    "decoder_list = [low_dim, 1000, 2000]\n",
    "encoder_activation = torch.nn.Tanh\n",
    "decoder_activation = torch.nn.Tanh\n",
    "\n",
    "u_ae = AE(encoder_list, decoder_list, encoder_activation(), decoder_activation(), \n",
    "          [1000, 1e-5], optimizer=optim, frequency_print=20)\n",
    "\n",
    "# Define the ANN model\n",
    "u_ann = ANN([5, 20], nn.Tanh(), [2000,1e-5])\n",
    "\n",
    "# Perform PODI on the pressure data\n",
    "u_rom = PODI_rom(u_ae, u_ann, train_design_parameters, train_velocity)\n",
    "\n",
    "# Predict the pressure data\n",
    "u_predict = u_rom.predict(test_design_parameters)\n",
    "\n",
    "# calculate the reconstrcuted error for each test data using mean squared error\n",
    "u_reconstructed_error = np.linalg.norm(test_velocity - u_predict.snapshots_matrix, axis=1) / np.linalg.norm(test_velocity, axis=1)\n",
    "\n",
    "# plot the error\n",
    "plt.bar([i for i in range(len(u_reconstructed_error))], u_reconstructed_error)\n",
    "plt.title('Velocity reconstructed error')\n",
    "plt.xlabel('Test data')\n",
    "plt.ylabel('Reconstructed error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VTK visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a funtion to write the data into a VTK file\n",
    "def vtk_writer(field_data, field_name, data_type,\n",
    "               refVTM, save_path_name, points_data = None):\n",
    "    # Add velocity data to each block within the MultiBlock dataset\n",
    "    \n",
    "    for block_i in range(refVTM.n_blocks):\n",
    "        block = refVTM[block_i]\n",
    "        if block is not None:\n",
    "            if data_type == 'scalar':\n",
    "                for data_i in range(len(field_name)):\n",
    "                    block.cell_data[field_name[data_i]] = field_data[data_i]\n",
    "            elif data_type == 'vector':\n",
    "                for data_i in range(len(field_name)):\n",
    "                    field = field_data[data_i].reshape(3, -1).T\n",
    "                    block.cell_data[field_name[data_i]] = field \n",
    "            if points_data is not None:\n",
    "                points = points_data.reshape(3, -1).T\n",
    "                block.points = points\n",
    "\n",
    "    # Save the modified VTM file\n",
    "    output_vtm_file_path = f'{save_path_name}.vtm'\n",
    "    refVTM.save(output_vtm_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the truth data, reconstructed data, and error into a VTK file\n",
    "# Load the reference mesh VTM file\n",
    "refVTM = pv.MultiBlock('simulation_data/refCase/VTK/refCase_0.vtm')\n",
    "field_name = ['truth', 'reconstructed', 'error']\n",
    "p_error_field = test_pressure - p_predict.snapshots_matrix\n",
    "\n",
    "# loop all test data and write the data into VTK file\n",
    "for i in range(len(test_pressure)):\n",
    "    vtk_writer([test_pressure[i], p_predict.snapshots_matrix[i], p_error_field[i]], \n",
    "                field_name, 'scalar', refVTM, f'visualization_data/EZyRB_AE_ANN/test_case_{i}_pressure', test_points[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the velocity data into VTK file\n",
    "field_name = ['truth', 'reconstructed', 'error']\n",
    "u_error_field = test_velocity - u_predict.snapshots_matrix\n",
    "\n",
    "# loop all test data and write the data into VTK file\n",
    "for i in range(len(test_velocity)):\n",
    "    vtk_writer([test_velocity[i], u_predict.snapshots_matrix[i], u_error_field[i]], \n",
    "                field_name, 'vector', refVTM, f'visualization_data/EZyRB_AE_ANN/test_case_{i}_velocity', test_points[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomParametric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
