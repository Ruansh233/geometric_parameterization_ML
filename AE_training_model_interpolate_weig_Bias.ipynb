{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd7a129e030>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import copy\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Prepare Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data preparation steps are done here\n",
    "num_simulations = 100\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Load npy data from training_data folder\n",
    "pressure_data = np.load('training_data/pressure_data.npy')\n",
    "velocity_data = np.load('training_data/velocity_data.npy')\n",
    "\n",
    "pressure_flattened = pressure_data.reshape(num_simulations, -1)\n",
    "velocity_flattened = velocity_data.reshape(num_simulations, -1)\n",
    "\n",
    "data_flattened = np.hstack((pressure_flattened, velocity_flattened))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(data_flattened, dtype=torch.float32)\n",
    "\n",
    "# Split the data\n",
    "train_size = int(train_ratio * num_simulations)\n",
    "test_size = num_simulations - train_size\n",
    "train_dataset, test_dataset = random_split(data_tensor, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Define the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "# Define model, loss, and optimizer\n",
    "input_dim = data_flattened.shape[1]  # 15000\n",
    "latent_dim = 256\n",
    "autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2053, Val Loss: 0.0778\n",
      "Epoch 2/20, Train Loss: 0.0549, Val Loss: 0.0430\n",
      "Epoch 3/20, Train Loss: 0.0480, Val Loss: 0.0563\n",
      "Epoch 4/20, Train Loss: 0.0570, Val Loss: 0.0578\n",
      "Epoch 5/20, Train Loss: 0.0574, Val Loss: 0.0561\n",
      "Epoch 6/20, Train Loss: 0.0554, Val Loss: 0.0531\n",
      "Epoch 7/20, Train Loss: 0.0522, Val Loss: 0.0491\n",
      "Epoch 8/20, Train Loss: 0.0462, Val Loss: 0.0281\n",
      "Epoch 9/20, Train Loss: 0.0379, Val Loss: 0.0258\n",
      "Epoch 10/20, Train Loss: 0.0277, Val Loss: 0.0267\n",
      "Epoch 11/20, Train Loss: 0.0239, Val Loss: 0.0198\n",
      "Epoch 12/20, Train Loss: 0.0189, Val Loss: 0.0167\n",
      "Epoch 13/20, Train Loss: 0.0163, Val Loss: 0.0164\n",
      "Epoch 14/20, Train Loss: 0.0163, Val Loss: 0.0162\n",
      "Epoch 15/20, Train Loss: 0.0160, Val Loss: 0.0157\n",
      "Epoch 16/20, Train Loss: 0.0155, Val Loss: 0.0152\n",
      "Epoch 17/20, Train Loss: 0.0151, Val Loss: 0.0150\n",
      "Epoch 18/20, Train Loss: 0.0148, Val Loss: 0.0147\n",
      "Epoch 19/20, Train Loss: 0.0147, Val Loss: 0.0146\n",
      "Epoch 20/20, Train Loss: 0.0145, Val Loss: 0.0145\n"
     ]
    }
   ],
   "source": [
    "# Training the Autoencoder\n",
    "model_save_path = 'model_saved'\n",
    "\n",
    "num_epochs = 20\n",
    "best_model_wts = copy.deepcopy(autoencoder.state_dict())\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        inputs = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(autoencoder.state_dict())\n",
    "\n",
    "# Load the best model weights\n",
    "autoencoder.load_state_dict(best_model_wts)\n",
    "\n",
    "# Save the trained autoencoder\n",
    "torch.save(autoencoder.state_dict(), f'{model_save_path}/autoencoder_10epoches.pth')\n",
    "\n",
    "# Extract the encoder\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Extract and store the decoder weights\n",
    "decoder_weights = []\n",
    "for i in range(num_simulations):\n",
    "    autoencoder.decoder.load_state_dict(autoencoder.decoder.state_dict())\n",
    "    decoder_weights.append(copy.deepcopy(autoencoder.decoder.state_dict()))\n",
    "\n",
    "# Save the decoder weights for future use\n",
    "torch.save(decoder_weights, f'{model_save_path}/decoder_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train RBF Interpolator on Decoder Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import Rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load design parameters from training_data folder\n",
    "design_parameters = np.load('training_data/displacement_data.npy')\n",
    "design_parameters_tensor = torch.tensor(design_parameters, dtype=torch.float32)\n",
    "\n",
    "# Load the stored decoder weights\n",
    "decoder_weights = torch.load(f'{model_save_path}/decoder_weights.pth')\n",
    "\n",
    "# Flatten the decoder weights for RBF interpolation\n",
    "flattened_weights = []\n",
    "for weights in decoder_weights:\n",
    "    flat_weights = []\n",
    "    for param in weights.values():\n",
    "        flat_weights.append(param.view(-1).numpy())\n",
    "    flattened_weights.append(np.concatenate(flat_weights))\n",
    "\n",
    "flattened_weights = np.array(flattened_weights)\n",
    "\n",
    "# Train the RBF interpolator\n",
    "rbf = Rbf(*design_parameters_tensor.T.numpy(), flattened_weights.T, function='linear', mode='N-D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interpolate New Decoder Weights for New Design Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygem import FFD\n",
    "\n",
    "def generate_new_design_parameters():\n",
    "\n",
    "    ffd = FFD([5, 5, 2])\n",
    "\n",
    "    # create the bounding box\n",
    "    ffd.box_origin = np.array([-0.01, -0.01, -0.001])\n",
    "    ffd.box_length = np.array([0.02, 0.02, 0.002])\n",
    "\n",
    "    # define the weight matrix. The boundary control points are fixed\n",
    "    weights = np.zeros(ffd.array_mu_x.shape)\n",
    "    weights[1:-1, 1:-1, :] = 1\n",
    "\n",
    "    # define the number of mesh samples, and the displacement ratio for the control points \n",
    "    disp_ratio=0.1\n",
    "\n",
    "    ffd.array_mu_x=disp_ratio*np.random.uniform(-1, 1, size=ffd.array_mu_x.shape)*weights\n",
    "    ffd.array_mu_y=disp_ratio*np.random.uniform(-1, 1, size=ffd.array_mu_x.shape)*weights\n",
    "\n",
    "    ffd.array_mu_x[:, :, 0] = ffd.array_mu_x[:, :, 1]\n",
    "    ffd.array_mu_y[:, :, 0] = ffd.array_mu_y[:, :, 1]\n",
    "\n",
    "    displacement_data = np.array([ffd.array_mu_x, ffd.array_mu_y, ffd.array_mu_z]).reshape(1, -1)\n",
    "\n",
    "    return displacement_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example new design parameter\n",
    "new_design_param = generate_new_design_parameters()\n",
    "\n",
    "# Predict new decoder weights using the RBF interpolator\n",
    "new_flattened_weights = rbf(*new_design_param)\n",
    "\n",
    "# Reshape the interpolated weights back to the original shapes\n",
    "def reshape_weights(flat_weights, template_weights):\n",
    "    new_weights = {}\n",
    "    start = 0\n",
    "    for key, param in template_weights.items():\n",
    "        param_shape = param.shape\n",
    "        param_size = param.numel()\n",
    "        new_weights[key] = torch.tensor(flat_weights[start:start + param_size]).view(param_shape)\n",
    "        start += param_size\n",
    "    return new_weights\n",
    "\n",
    "# Use the first set of decoder weights as a template\n",
    "template_weights = decoder_weights[0]\n",
    "new_decoder_weights = reshape_weights(new_flattened_weights, template_weights)\n",
    "\n",
    "# Load the interpolated weights into a new decoder\n",
    "new_autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "new_autoencoder.decoder.load_state_dict(new_decoder_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Reconstruct Using the New Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the latent representation of the data_tensor\n",
    "with torch.no_grad():\n",
    "    latent_representation = autoencoder.encoder(data_tensor)  \n",
    "\n",
    "# Reconstruct the data using the new decoder\n",
    "with torch.no_grad():\n",
    "    reconstructed_data = new_autoencoder.decoder(latent_representation)\n",
    "\n",
    "# Reshape the reconstructed data if needed\n",
    "reconstructed_data_reshaped = reconstructed_data.view(-1, 3)  # Example reshape if needed\n",
    "print(reconstructed_data_reshaped)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomParametric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
